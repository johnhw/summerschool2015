{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section I: Concepts\n",
    "\n",
    "I'm [John Williamson](http://johnhw.com)\n",
    "\n",
    "The purpose of this crash course is to give you enough vocabulary to be able to follow the rest of the summer school. There isn't time to cover the details of the methods I'll talk about, the historical orgins and background or much about what techinques and models you might prefer and why. \n",
    "\n",
    "Instead, I aim to cover just enough that you can understand the material that follows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is machine learning?\n",
    "\n",
    "### Machine learning can be summarised as making *predictions* from *data*\n",
    "This is slightly distinct from statistics, which is traditionally concerned with making inferences from data -- e.g. determining if an effect is present in a scientific study. Instead, machine learning tries to estimate unknown variables given some data which might predict them. \n",
    "\n",
    "### Machine learning in HCI\n",
    "#### Example: gesture recognition\n",
    "#### Example: language modelling\n",
    "#### Example: touch prediction\n",
    "#### Example: sensor fusion\n",
    "\n",
    "### Some mathematical notation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.axes3d as axes3d\n",
    "import seaborn\n",
    "import sklearn.preprocessing, sklearn.cluster, sklearn.tree, sklearn.neighbors, sklearn.ensemble\n",
    "import ipy_table\n",
    "import sklearn.svm, sklearn.cross_validation, sklearn.grid_search, sklearn.metrics, sklearn.datasets, sklearn.decomposition, sklearn.manifold\n",
    "import pandas as pd\n",
    "# force plots to appear inline on this page\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised versus unsupervised learning\n",
    "\n",
    "Supervised learning involves learning a relationship between attribute variables and target variables; in other words learning a function which maps input measurements to target values. This can be in the context of making discrete decisions (is this image a car or not?) or learning continuous relationships (\n",
    "\n",
    "### Supervised learning\n",
    "#### Classification\n",
    "#### Knn example\n",
    "\n",
    "#### Regression\n",
    "### Unsupervised learning\n",
    "Unsupervised learning learns the structure of data without any explicit labeling of points. The key idea is that datasets may have a simple underlying or *latent* representation which can be determined simply by looking at the data itself.\n",
    "\n",
    "Two common unsupervised learning tasks are *clustering* and *dimensional reduction*. Clustering can be thought of as the unsupervised analogue of classification -- finding discrete classes in data. Dimensional reduction can be thought of as the analogue of regression -- finding a small set of continuous variables which \"explain\" a higher dimensional set. \n",
    "\n",
    "#### Learning-to-rank\n",
    "\n",
    "#### Generative models versus black box models\n",
    "##### Tank recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensional reduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal component analysis\n",
    "One very simple method of dimensional reduction is *principal component analysis*. This is a linear method; in other words it finds rigid rotations and scalings of the data to project it onto a lower dimension.\n",
    "\n",
    "The PCA algorithm effectively looks for the rotation that makes the dataset look \"fattest\" (maximises the variance), chooses that as the first dimension, then removes that dimension, rotates again to make it look \"fattest\" and repeats. Linear algebra makes it efficient to do this process in a single step by extracting the *eigenvectors* of the *covariance matrix*. \n",
    "\n",
    "PCA always finds a matrix $A$ such that $y = Ax$, where the dimension of $y<x$. PCA is exact and repeatable and very efficient, but it can only find rigid transformations of the data. This is a limitation of any linear dimensional reduction technique.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digits = sklearn.datasets.load_digits()\n",
    "digit_data = digits.data\n",
    "\n",
    "# plot a single digit data element\n",
    "def show_digit(d):\n",
    "    plt.imshow(d.reshape(8,8), cmap='gray', interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.figure()\n",
    "    \n",
    "# show a couple of raw digits\n",
    "show_digit(digit_data[0])\n",
    "show_digit(digit_data[400])\n",
    "\n",
    "# apply principal component analysis\n",
    "pca = sklearn.decomposition.PCA(n_components=2).fit(digit_data)\n",
    "digits_2d = pca.transform(digit_data)\n",
    "\n",
    "# plot each digit with a different color\n",
    "plt.scatter(digits_2d[:,0], digits_2d[:,1], c=digits.target, cmap='jet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful property of PCA is that we compute exactly how \"fat\" each of these learned dimensions were. The ratio of *explained variance* tells us how much each of the original variation in the dataset is captured by each learned dimension. \n",
    "\n",
    "If most of the variance is in the first couple of components, we know that a 2D representation will capture much of the original dataset. If the ratios of variance are spread out over many dimensions, we will need many dimensions to represent the data well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can see how many dimensions we need to represent the data well using the eigenspectrum\n",
    "# here we show the first 32 components\n",
    "pca = sklearn.decomposition.PCA(n_components=32).fit(digit_data)\n",
    "plt.bar(np.arange(32), pca.explained_variance_ratio_)\n",
    "plt.xlabel(\"Component\")\n",
    "plt.ylabel(\"Proportion of variance explained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example where PCA does badly is the \"swiss roll dataset\" -- a plane rolled up into a spiral in 3D. This has a very simple structure; a simple plane with some distortion. But PCA can never unravel the spiral to find this simple explanation because it cannot be unravelled via a linear transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "swiss_pos, swiss_val = sklearn.datasets.make_swiss_roll(800, noise=0.0)\n",
    "fig = plt.figure()\n",
    "# make a 3D figure\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(swiss_pos[:,0], swiss_pos[:,1], swiss_pos[:,2], c=swiss_val, cmap='gist_heat', s=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply PCA to learn this structure (which doesn't help much)\n",
    "pca = sklearn.decomposition.PCA(2).fit(swiss_pos)\n",
    "pca_pos = pca.transform(swiss_pos)\n",
    "plt.scatter(pca_pos[:,0], pca_pos[:,1], c=swiss_val, cmap='gist_heat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manifold learning\n",
    "Other approaches to dimensional reduction look at the problem in terms of learning a *manifold*. A *manifold* is a geometrical structure which is locally like a low-dimensional Euclidean space. Examples are the plane rolled up in the swiss roll, or a 1D \"string\" tangled up in a 3D space. \n",
    "\n",
    "Manifold approaches attempt to automatically find these smooth embedded structures by examining the local structure of datapoints (often by analysing the nearest neighbour graph of points). This is more flexible than linear dimensional reduction as it can in theory unravel very complex or tangled datasets. \n",
    "\n",
    "However, the algorithms are usually approximate, they do not give guarantees that they will find a given manifold, and can be computationally intensive to run.\n",
    "\n",
    "A popular manifold learning algorithm is *ISOMAP* which uses nearest neighbour graphs to identify locally connected parts of a dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "isomap_pos = sklearn.manifold.Isomap(n_neighbors=10, n_components=2).fit_transform(swiss_pos)\n",
    "plt.scatter(isomap_pos[:,0], isomap_pos[:,1], c=swiss_val, cmap='gist_heat')\n",
    "plt.figure()\n",
    "\n",
    "# note that isomap is sensitive to noise!\n",
    "noisy_swiss_pos, swiss_val = sklearn.datasets.make_swiss_roll(800, noise=0.5)\n",
    "isomap_pos = sklearn.manifold.Isomap(n_neighbors=10, n_components=2).fit_transform(noisy_swiss_pos)\n",
    "plt.scatter(isomap_pos[:,0], isomap_pos[:,1], c=swiss_val, cmap='gist_heat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply a more modern dimensional reduction method -- t-distributed stochastic neighbour embedding -- to the digits dataset we used in the PCA example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is very slow to run\n",
    "tsne_digits = sklearn.manifold.TSNE(n_components=2).fit_transform(digit_data)\n",
    "plt.scatter(tsne_digits[:,0], tsne_digits[:,1], c=digits.target, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE separates out the digits very nicely into separate regions one for each digit, from the image data alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering\n",
    "Clustering tries to find well-seperated (in some sense) partitions of a data set. It is essentially a search for natural boundaries in the data. An example use of clustering might be gathering data from a large number of plants (e.g. measuring flower sizes, colouring, leaf symmetries, etc.) and then applying clustering to the data to try and separate out potential species groupings. \n",
    "\n",
    "There are many clustering approaches. A simple one is *k-means*, which finds clusters via an iterative algortihm. The number of clusters must be chosen in advance. In general, it is hard to estimate the number of clusters, although there are algorithms for estimating this. k-means proceeds by choosing a set of $k$ random points as initial cluster seed points; classifiying each data point according to its nearest seed point; then moving the cluster point towards the mean position of all the data points that belong to it. \n",
    "\n",
    "The k-means algorithm does not guarantee to find the best possible clustering -- it falls into *local minima*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is the classic \"Iris\" dataset, that captures 4 attributes measured from various irises (of three distinct species)\n",
    "iris = sklearn.datasets.load_iris()\n",
    "\n",
    "# we plot the dataset on two of the dimensions\n",
    "plt.scatter(iris.data[:,2], iris.data[:,1], c=iris.target, cmap='jet')\n",
    "\n",
    "plt.figure()\n",
    "# now we cluster *without* any knowledge of the true class labels\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=3)\n",
    "kmeans_target = kmeans.fit_predict(iris.data)\n",
    "plt.scatter(iris.data[:,2], iris.data[:,1], c=kmeans_target, cmap='jet')\n",
    "# plot the cluster centres\n",
    "plt.plot(kmeans.cluster_centers_[:,2], kmeans.cluster_centers_[:,1], 'r*', markersize=20)\n",
    "\n",
    "## if we get the number of clusters wrong, the algorithm hallucinates clusters where there are none:\n",
    "plt.figure()\n",
    "# now we try and find 5 clusters\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=5)\n",
    "kmeans_target = kmeans.fit_predict(iris.data)\n",
    "plt.scatter(iris.data[:,2], iris.data[:,1], c=kmeans_target, cmap='jet')\n",
    "# plot the cluster centres\n",
    "plt.plot(kmeans.cluster_centers_[:,2], kmeans.cluster_centers_[:,1], 'r*', markersize=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative clustering\n",
    "Another approach is to consider the clustering from bottom-up instead of top-down. *Agglomerative clustering* merges pairs of data points together, creating a tree of neighbour clusters. Once the preset number of leaves have been formed, these are returned as the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we plot the dataset on two of the dimensions\n",
    "plt.scatter(iris.data[:,2], iris.data[:,1], c=iris.target, cmap='jet')\n",
    "\n",
    "plt.figure()\n",
    "agg = sklearn.cluster.AgglomerativeClustering(n_clusters=3)\n",
    "agg_target = agg.fit_predict(iris.data)\n",
    "plt.scatter(iris.data[:,2], iris.data[:,1], c=agg_target, cmap='jet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features, labels, data formats\n",
    "\n",
    "### Types of data\n",
    "#### Continuous\n",
    "#### Ordinal\n",
    "#### Categorical\n",
    "\n",
    "\n",
    "### Feature vectors\n",
    "#### Vector length\n",
    "#### Selecting or generating features\n",
    "#### Dealing with time series, text, images, sounds\n",
    "### Labels\n",
    "#### Multi-class approaches\n",
    "###### One-vs-alll\n",
    "###### One-vs-one\n",
    "\n",
    "#### One-hot encoding\n",
    "#### Hyperparameters\n",
    "##### Learning rate\n",
    "##### Smoothness\n",
    "##### Momentum\n",
    "##### Grid search\n",
    "##### Using grid seaches effectively\n",
    "\n",
    "### Learning\n",
    "#### Batches, online learning, mini-batches\n",
    "#### Loss functions\n",
    "#### Gradient descent \n",
    "##### Stochastic gradient descent\n",
    "#### Unbalanced data\n",
    "#### Solutions for unbalanced data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating machine learning\n",
    "\n",
    "\n",
    "\n",
    "### Classifiers\n",
    "#### Accuracy\n",
    "#### Why is accuracy not enough?\n",
    "#### FAR, FRR, F1-score\n",
    "#### Receiver-operator curves\n",
    "#### EER, AUC\n",
    "#### Confusion matrices\n",
    "\n",
    "### Unsupervised learning\n",
    "#### Clustering metrics\n",
    "#### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd datasets/uci\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the \"Sonar\" data set [https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)]\n",
    "# This is a set of 60 sonar measurements, from bouncing sonar waves off either rocks (\"R\") or mines (\"M\")\n",
    "# The problem is to tell the mines apart from the rocks\n",
    "sonar_data = pd.read_csv(\"sonar.all-data\")\n",
    "\n",
    "# separate features\n",
    "sonar_features = np.array(sonar_data)[:,0:60].astype(np.float64)\n",
    "\n",
    "# we use label_binarize to convert \"M\" and \"R\" labels into {0,1}\n",
    "# the ravel() just flattens the resulting 1D matrix into a vector\n",
    "sonar_labels = sklearn.preprocessing.label_binarize(np.array(sonar_data)[:,60], classes=['M', 'R']).ravel()\n",
    "\n",
    "# split into a train and test section, holding out 20% (0.2) of the data for testing\n",
    "sonar_train_features, sonar_test_features, sonar_train_labels, sonar_test_labels = sklearn.cross_validation.train_test_split(\n",
    "    sonar_features, sonar_labels, test_size=0.3, random_state=0)\n",
    "\n",
    "# fit an SVM with the default parameters\n",
    "svm = sklearn.svm.SVC(C=1)\n",
    "svm.fit(sonar_train_features, sonar_train_labels)\n",
    "print \"Score: %f\" % svm.score(sonar_test_features, sonar_test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can print the confusion matrix\n",
    "sonar_predicted_labels = svm.predict(sonar_test_features)\n",
    "confusion_matrix =  sklearn.metrics.confusion_matrix(sonar_test_labels, sonar_predicted_labels)\n",
    "\n",
    "# ipy_table.make_table just pretty prints the resulting matrix\n",
    "ipy_table.make_table(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can plot the receiver-operator curve: the graph of false positive rate against true positive rate\n",
    "scores = svm.decision_function(sonar_test_features)\n",
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(sonar_test_labels, scores)\n",
    "plt.plot(fpr,tpr)\n",
    "plt.plot([0,1], [0,1])\n",
    "plt.plot([0,1], [1,0])\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.legend([\"ROC\", \"Chance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"AUC=%f\" % sklearn.metrics.roc_auc_score(sonar_test_labels, scores)\n",
    "print \"Accuracy=%f\" % sklearn.metrics.accuracy_score(sonar_test_labels, sonar_predicted_labels)\n",
    "# recall -- how many of the positive samples did it find?\n",
    "print \"Recall=%f\" % sklearn.metrics.recall_score(sonar_test_labels, sonar_predicted_labels)\n",
    "# precision  -- how many of the negative samples did it reject?\n",
    "print \"Precision=%f\" % sklearn.metrics.precision_score(sonar_test_labels, sonar_predicted_labels)\n",
    "\n",
    "# F1 =  2 * (precision * recall) / (precision + recall)\n",
    "print \"F1 score=%f\" % sklearn.metrics.f1_score(sonar_test_labels, sonar_predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sklearn.metrics.classification_report(sonar_test_labels, sonar_predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# let's search for the best parameters of C and gamma\n",
    "# we create a grid of parameter values, log-spaced across a wide range\n",
    "C_vals = np.logspace(-2,8,10)\n",
    "gamma_vals = np.logspace(-4,3,10)\n",
    "\n",
    "# 10x10 values of C and gamma; 10 fold cross validation for each\n",
    "grid_cv = sklearn.grid_search.GridSearchCV(svm, {'C':C_vals, 'gamma':gamma_vals}, cv=10)\n",
    "grid_cv.fit(sonar_train_features, sonar_train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the optimal best results\n",
    "print grid_cv.best_params_\n",
    "print grid_cv.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit an SVM using the optimal parameters found\n",
    "svm = sklearn.svm.SVC(C=grid_cv.best_params_['C'], gamma=grid_cv.best_params_['gamma'])\n",
    "svm.fit(sonar_train_features, sonar_train_labels)\n",
    "print \"Score: %f\" % svm.score(sonar_test_features, sonar_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can randomly permute the label vectors using np.random.permutation\n",
    "# Now they have no relation to the training features\n",
    "random_sonar_train_labels = np.random.permutation(sonar_train_labels)\n",
    "random_sonar_test_labels = np.random.permutation(sonar_test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit an SVM to the randomised label data\n",
    "svm = sklearn.svm.SVC(C=15, gamma=0.12)\n",
    "svm.fit(sonar_train_features, random_sonar_train_labels)\n",
    "# this should be about 0.5\n",
    "print \"Score: %f\" % svm.score(sonar_test_features, random_sonar_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "Overfitting is the key issue with machine learning algorithms. It is trivially easy to devise a supervised learning algorithm that takes in input features and exactly predicts the corresponding output classes *in the training data*. A simple lookup table will do this.\n",
    "\n",
    "To make useful predictions, a learning algorithm must predict values with features it has never seen. The problem is we can only optimise the performance based on data we *have* seen. The **generalisation** performance of an algorithm is the ablility to make predictions outside of the training data. \n",
    "\n",
    "This means that we cannot optimise an algorithm by adjusting parameters to fit the training data better; this will lead to **overfitting**, where the predictive power of the algorithm *decreases* as it is exposed to additional data. \n",
    "\n",
    "Instead, we must use a separate testing set which the training algorithm is never exposed to to evaluate performance. The importance of this can be shown in a few simple examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial overfitting\n",
    "A simple example of overfitting can be seen when fitting a polynomial $y=a_0x^0 + a_1x^1 + \\dots + a_qx^q$ to $n$ points. When $n=q$, the curve goes through all the points, but makes wildly inappropriate interpolations between them.\n",
    "\n",
    "The example below shows this effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create some random points\n",
    "n_points = 6\n",
    "x = np.random.uniform(-1,1,(n_points,))\n",
    "y = np.random.uniform(-1,1,(n_points,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the points\n",
    "plt.scatter(x,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a short function to fit and plot a polynomial fit . NumPy has a built in function `polyfit` which does least-squares fitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_poly_fit(x,y,order):    \n",
    "    plt.figure()\n",
    "    plt.scatter(x,y)\n",
    "    # fit a polynomial\n",
    "    coeffs = np.polyfit(x,y,order)\n",
    "    # evaluate it across the domain\n",
    "    xs = np.linspace(np.min(x),np.max(x),100)\n",
    "    ys = np.polyval(coeffs, xs)\n",
    "    plt.plot(xs,ys)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_poly_fit(x,y,order=1) # order=1 means a linear fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the above code with increasing values for the order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel density estimation example\n",
    "We can see the same effect if we try to learn the distribution of data using **kernel density estimation** (KDE). KDE is effectively a smoothed histogram, which is created by \"placing\" smooth distributions on each observed data point and summing them (i.e. convolving them with some window function). This can be used to estimate an underlying smooth distribution from point samples.\n",
    "\n",
    "The key parameter in KDE is the **kernel width** $\\sigma$, which determines how wide each distribution will be and thus how smooth the overall distribution will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate some random numbers -- the use of the  beta distribution isn't important, it just gives an interesting shape\n",
    "x = np.random.beta(0.5, 2, 40);\n",
    "# plot the data points\n",
    "plt.figure()\n",
    "# scatter plot showing the actual positions\n",
    "plt.scatter(x,np.ones_like(x))\n",
    "plt.figure()\n",
    "# histogram of the data points\n",
    "plt.hist(x, normed=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats \n",
    "\n",
    "# plot the kernel density estimate (Gaussian window) with the given bandwidth\n",
    "def plot_kde(x, width):\n",
    "    kde = stats.kde.gaussian_kde(x, bw_method=width)\n",
    "    # evaluate kde estimate over range of x\n",
    "    xs = np.linspace(np.min(x), np.max(x), 100) \n",
    "    plt.figure()\n",
    "    plt.plot(xs, kde(xs))\n",
    "    plt.scatter(x, np.ones_like(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_kde(x, 2) # too smooth\n",
    "plot_kde(x, 1) # a bit too smooth\n",
    "plot_kde(x, 0.5) # good\n",
    "plot_kde(x, 0.1) # too rough\n",
    "plot_kde(x, 0.01) # just learning the data points\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the function approximates the data better, the generalisation performance drops. If we split the data randomly into two portions $X_1$ and $X_2$, and learn the KDE using only $X_1$ (training set) and then compute how likely $X_2$ (test set) is given that learned distribution, we can see this loss of generalisation performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(x):\n",
    "    # our data here is random and uncorrelated, so we can just split the array into two\n",
    "    l = len(x)//2\n",
    "    return x[:l], x[l:]\n",
    "\n",
    "def learn_kde(x, width):\n",
    "    return stats.kde.gaussian_kde(x,width)\n",
    "\n",
    "def evaluate_kde(x, kde):\n",
    "    # we can compute the log-likelihood by summing the log pdf evaluated at x\n",
    "    return np.sum(kde.logpdf(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_kde(x, width):\n",
    "    # split the data into two parts, train on one, and then test it on both of the splits\n",
    "    x1,x2 = split_data(x)\n",
    "    kde = learn_kde(x1, width)\n",
    "    # return the train log-likelihood and test log-likelihood    \n",
    "    return evaluate_kde(x1, kde), evaluate_kde(x2, kde)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_kde_lik(x):\n",
    "    # plot test and train log-likelihood as a function of 1/sigma\n",
    "    widths = np.linspace(0.05, 20, 100)\n",
    "    trains = []\n",
    "    tests = []\n",
    "    # test a bunch of widths\n",
    "    for width in widths:\n",
    "        train, test = test_kde(x,1.0/width)\n",
    "        trains.append(train)\n",
    "        tests.append(test)\n",
    "        \n",
    "    # plot and label\n",
    "    plt.plot(widths, trains)\n",
    "    plt.plot(widths, tests)\n",
    "    plt.xlabel(\"$\\sigma^{-1}$\")\n",
    "    plt.ylabel(\"Log-likelihood\")\n",
    "    plt.legend([\"Training\", \"Test\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the log-likelihood as a function of $\\sigma^{-1}$ (the reciprocal simply makes it easier to see what is going on in the graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_kde_lik(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training error can always be reduced -- but it makes things worse\n",
    "We can see that test and training become better as $\\sigma^{-1}$ approaches 2-4 (the exact value will depend on what random numbers we originally drew), then rapidly decreases; but the training log-likelihood **always** increases as we approximate the original data better and better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data hygiene\n",
    "This is why **data hygiene** is absolutely critical. If you let any part of the data you use to evaluate performance affect the train process your results are *meaningless*. \n",
    "\n",
    "#### Randomised selection\n",
    "One approach to splitting up data is to randomly assign some elements to the training set and some to the test set (e.g. in an image classification task, 70% of the images are assigned to the training class and 30% to the test class). \n",
    "\n",
    "This seems like an unbiased way of separating the data, and it is for problems which are effectively uncorrelated. But imagine we have a time series $x_0, x_1, \\dots, x_n$ and we build our input features $X_0, X_1, \\dots$ by taking overlapping windows of the series. If we randomly choose elements of $X$, many of the elements in the test and training set may be almost identical ( because they appeared next to each other in the time series). This leads to wildly optimistic test results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Block selection\n",
    "A much better approach here is to split the data into a large chunks. Say the data was a series of field recordings of birds taken on 10 different days; the first 6 days might be assigned as training and the last 4 as test. \n",
    "\n",
    "This is much more likely to be a reliable estimator of future performance, because the key idea is to *predict future behaviour* -- to learn what we have not seen. Choosing the split of training and test requires thought and domain knowledge and it is critical to make sure that the evaluation results are meaningful.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "Sometimes the data is too precious to split into training and test sets; there simply isn't enough of it. \n",
    "\n",
    "One approach to getting reliable results without overfitting is to use *cross validation*. This simply involves splitting the data into a test and training set repeatedly and averaging the results. \n",
    "\n",
    "#### k-fold cross validation\n",
    "*k-fold* cross validation splits the data into $k$ blocks (again the block selection strategy needs to be chosen carefully to avoid overfitting) and then trains on $k-1$ of the blocks and test on the remaining one, for each of the $k$ test blocks.\n",
    "#### Leave-one-out cross validation (LOOCV)\n",
    "LOOCV takes this to an extreme, and splits the data into $k=n$ blocks (for $n$ data points) and trains on all but one data point and tests on the one that was left out. This is a reliable estimator of performance but can be very computationally expensive, and the benefits over $k$-fold cross validation are not always great.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# k-fold cross validation with the KDE example\n",
    "\n",
    "def k_split(x, k):\n",
    "    # split the data into even chunks\n",
    "    l = len(x)//k\n",
    "    return [x[i*l:(i+1)*l] for i in range(k)]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_kde_k_fold(x, width, k):\n",
    "    # split the data into two parts, train on one, and then test it on both of the splits\n",
    "    liks = []\n",
    "    folds = k_split(x,k)\n",
    "    for kn in range(k):\n",
    "        # concatenate all folds *but* kn\n",
    "        x1 = np.concatenate([folds[i] for i in range(k) if i!=kn])            \n",
    "        # test set is the one we left out\n",
    "        x2 = folds[kn]\n",
    "        # fit the KDE\n",
    "        kde = learn_kde(x1, width)\n",
    "        # return the log-likelihood for this fold\n",
    "        lik = evaluate_kde(x2, kde)\n",
    "        liks.append(lik)\n",
    "    return liks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now we can plot the performance with one std. dev. bounds\n",
    "\n",
    "def plot_kde_lik_k_fold(x, k):\n",
    "    # plot test and train log-likelihood as a function of 1/sigma\n",
    "    widths = np.linspace(0.05, 20, 100)\n",
    "    means = []\n",
    "    stds = []\n",
    "    # test a bunch of widths\n",
    "    for width in widths:\n",
    "        ks = test_kde_k_fold(x,1.0/width,k)\n",
    "        means.append(np.mean(ks))\n",
    "        stds.append(np.std(ks))\n",
    "                \n",
    "    # plot and label    \n",
    "    means = np.array(means) # convert lists no numpy arrays so we can do arithmetic on them\n",
    "    stds = np.array(stds)\n",
    "    \n",
    "    plt.fill_between(widths, means-stds, means+stds, alpha=0.1)\n",
    "    plt.plot(widths, means)\n",
    "    plt.xlabel(\"$\\sigma^{-1}$\")\n",
    "    plt.ylabel(\"Log-likelihood\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_kde_lik_k_fold(x, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation sets\n",
    "Often your learning algorithms might have hyperparameters you need to estimate (e.g. learning rate). \n",
    "\n",
    "You absolutely **cannot** use the test set to evaluate the performance with various parameter settings and then choose the best one. This will overfit, because the test set is influencing the training performance. Train and test must be **completely** separated; no information may flow from the test data to the training process\n",
    "\n",
    "Instead, you can create a **validation** set from the training set, and use that to tweak the hyperparameters. One common and sensible approach is to first separate off a single, fixed test set, and then use cross-validation to create multiple train and validation sets. \n",
    "\n",
    "#### Nested cross-validation\n",
    "*Nested cross-validation* applies cross validation to both the validation and test sets, first fixing a fold split for the test set, then optimising the hyperparameters using cross-validation on the remaining data, then moving onto the next fold split for the test set, and so on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines\n",
    "Let's return to our classification example. What is good performance? Or more particularly, what is bad performance -- what's the baseline? A very simple test of whether the classifier is learning **anything** useful is to simply randomly permute the targets so they no longer correspond to the inputs. \n",
    "\n",
    "This estimates the chance performance of the classification process. It might be that you have a binary classification problem with exactly balanced datasets, so the baseline is 50%; but often the data is unbalanced and there are multiple classes. Permuting the targets is a very quick way to test the random performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An unbalanced example\n",
    "As a simple example, we can look at a very unbalanced classifcation problem. The trees dataset includes measurements from aerial photography of land cover. A small portion of it is diseased tree growth. We can train a classifier to predict the disease status from the measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the \"wilt\" data set https://archive.ics.uci.edu/ml/datasets/Wilt\n",
    "wilt_data = pd.read_csv(\"wilt.csv\")\n",
    "# separate features\n",
    "wilt_features = wilt_data.ix[:,1:].as_matrix()\n",
    "\n",
    "# we use label_binarize to convert \"M\" and \"R\" labels into {0,1}\n",
    "# the ravel() just flattens the resulting 1D matrix into a vector\n",
    "wilt_labels = sklearn.preprocessing.label_binarize(np.array(wilt_data.ix[:,0]), classes=['w', 'n']).ravel()\n",
    "\n",
    "# split into a train and test section, holding out 20% (0.2) of the data for testing\n",
    "wilt_train_features, wilt_test_features, wilt_train_labels, wilt_test_labels = sklearn.cross_validation.train_test_split(\n",
    "    wilt_features, wilt_labels, test_size=0.3, random_state=0)\n",
    "\n",
    "# fit an SVM with the default parameters\n",
    "wilt_svm = sklearn.svm.SVC(C=10000, gamma=5.0)\n",
    "wilt_svm.fit(wilt_train_features, wilt_train_labels)\n",
    "print \"Score: %f\" % wilt_svm.score(wilt_test_features, wilt_test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! 95% accuracy!\n",
    "Let's try it with random labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wilt_svm = sklearn.svm.SVC(C=1)\n",
    "wilt_svm.fit(wilt_train_features, np.random.permutation(wilt_train_labels))\n",
    "print \"Score: %f\" % wilt_svm.score(wilt_test_features, np.random.permutation(wilt_test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM learned essentially **nothing**; the accuracy is very misleading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = wilt_svm.decision_function(wilt_test_features)\n",
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(wilt_test_labels, scores)\n",
    "plt.plot(fpr,tpr)\n",
    "plt.plot([0,1], [0,1])\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.legend([\"ROC\", \"Chance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-algorithms\n",
    "Meta-algortihms are ways of combining multiple learning models to improve performance. \n",
    "These generally involve *ensembles* of classifiers. They can be applied to a very wide range of strategies, and they *generally always improve performance* (even if the improvement is marginal). In major machine learning competitions (e.g. Kaggle, the Netflix challenge) *ensemble* algorithms are almost always the top performers.\n",
    "\n",
    "The idea of a *ensemble* method is that if you train multiple classifiers/regressors of different types or on different datasets, they will learn different things well; and combining them together increases the robustness and generalisation performance. \n",
    "\n",
    "### Voting hybrid models\n",
    "One simple model is to train a number of different types of classifiers on a dataset and have them vote on the class label. For regression, the median or mean can be used as the combination function.\n",
    "\n",
    "#### Weighted models\n",
    "### Bagging\n",
    "#### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# voting model on the sonar dataset\n",
    "\n",
    "# Fit SVM\n",
    "svm = sklearn.svm.SVC(C=12, gamma=0.3)\n",
    "svm.fit(sonar_train_features, sonar_train_labels)\n",
    "print \"SVM Score: %f\" % svm.score(sonar_test_features, sonar_test_labels)\n",
    "\n",
    "# Fit KNN\n",
    "knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(sonar_train_features, sonar_train_labels)\n",
    "print \"KNN Score: %f\" % knn.score(sonar_test_features, sonar_test_labels)\n",
    "\n",
    "# Fit decision tree\n",
    "dec = sklearn.tree.DecisionTreeClassifier()\n",
    "dec.fit(sonar_train_features, sonar_train_labels)\n",
    "print \"DEC Score: %f\" %dec.score(sonar_test_features, sonar_test_labels)\n",
    "\n",
    "# predict labeels\n",
    "svm_labels = svm.predict(sonar_test_features)\n",
    "knn_labels = knn.predict(sonar_test_features)\n",
    "dec_labels = dec.predict(sonar_test_features)\n",
    "\n",
    "# to vote, we can take the mean and use 0.5 as a threshold\n",
    "mean_labels = np.mean(np.vstack((svm_labels, knn_labels, dec_labels)), axis=0)\n",
    "voted = np.where(mean_labels>0.5, 1, 0)\n",
    "\n",
    "print \"Voted score: %f\" % sklearn.metrics.accuracy_score(sonar_test_labels, voted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also *weight* classifiers by how well they are doing; strong classifiers should be weighted more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "Rather than combining different models, we can use a single model but different *datasets*. Bagging applies the statistical process called the *bootstrap* to generate multiple classifiers/regressors. \n",
    "\n",
    "Bootstrap generates *synthetic datasets* by randomly resampling the original dataset **with replacement**. From a dataset $X$ with $n$ elements, it generates $k$ new datasets each of which have $n$ elements consisting of random draws from the rows of $X$. Bagging just trains one classifier for each of these $k$ datasets then combines the output by voting or averaging. This increases the robustness of the model.\n",
    "\n",
    "This has the advantage of working for *any* supervised learning task (but there may be significant computational issues if the datasets are very large). However, bagging may not improve (or may even make worse) classifiers that are not already overfitting. \n",
    "\n",
    "Variations of this approach include randomly sampling the features (columns of $X$); random feature selection is called *random subspaces* and randomly sampling both features and samples is known as *random patches*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create some random points\n",
    "n_points = 10\n",
    "x = np.random.uniform(-1,1,(n_points,))\n",
    "y = np.random.uniform(-1,1,(n_points,))\n",
    "\n",
    "# we return to the polynomial fitting example\n",
    "def bootstrap_sample(x,y):\n",
    "    # resample with replacement\n",
    "    indices = np.random.randint(0, len(y), len(y))\n",
    "    return x[indices], y[indices]\n",
    "\n",
    "def plot_poly_fit_bagged(x,y,order,n_bags):    \n",
    "    plt.figure()\n",
    "    plt.scatter(x,y)\n",
    "    \n",
    "    all_ys = []\n",
    "    \n",
    "    # fit polynomials to resampled datasets\n",
    "    for i in range(n_bags):\n",
    "        bx, by = bootstrap_sample(x,y)\n",
    "        # fit a polynomial\n",
    "        coeffs = np.polyfit(bx,by,order)\n",
    "        \n",
    "        # evaluate it across the domain\n",
    "        xs = np.linspace(np.min(x),np.max(x),100)\n",
    "        ys = np.polyval(coeffs, xs)\n",
    "        all_ys.append(ys)\n",
    "    \n",
    "    # convert to array\n",
    "    all_ys = np.array(all_ys)\n",
    "    \n",
    "    for ys in all_ys:\n",
    "        plt.plot(xs,ys, alpha=0.05)\n",
    "    plt.plot(xs, np.median(all_ys, axis=0))\n",
    "    plt.ylim(-2,2)\n",
    "    \n",
    "plot_poly_fit(x,y, 4)\n",
    "plot_poly_fit_bagged(x,y,4,100)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can try bagging the mine data set\n",
    "\n",
    "# Fit poorly generalising KNN\n",
    "knn = sklearn.neighbors.KNeighborsClassifier(2)\n",
    "bagged_knn = sklearn.ensemble.BaggingClassifier(knn, n_estimators=50)\n",
    "\n",
    "knn.fit(sonar_train_features, sonar_train_labels)\n",
    "print \"KNN Score: %f\" % knn.score(sonar_test_features, sonar_test_labels)\n",
    "\n",
    "bagged_knn.fit(sonar_train_features, sonar_train_labels)\n",
    "print \"Bagged KNN Score: %f\" % bagged_knn.score(sonar_test_features, sonar_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forests\n",
    "*Random forests* use ensembles of very simple classifiers -- decision trees -- to transform a weak and poorly generalising learning model into a much more robust model. Random forests often have very competitive performance in classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the test 10 times\n",
    "for trials in range(20):\n",
    "    forest_scores = []\n",
    "    # iterate over number of trees in the ensemble\n",
    "    for estimators in range(20):\n",
    "        # completely randomised tree ensemble classifiers\n",
    "        forest = sklearn.ensemble.ExtraTreesClassifier(n_estimators=estimators+1)\n",
    "        forest.fit(sonar_train_features, sonar_train_labels)\n",
    "        forest_scores.append(forest.score(sonar_test_features, sonar_test_labels))\n",
    "    plt.plot(np.arange(len(forest_scores))+1, forest_scores, alpha=0.2)\n",
    "plt.xlabel('No. estimators')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting\n",
    "*Boosting* is an alternative ensemble method which trains a weak classifier on a dataset, identifies samples it is performing poorly on, and trains another classifier to learn the poor samples, identifies samples the ensemble is still performing poorly on, and trains a classifer to learn **those** and so on. \n",
    "\n",
    "The selection of the weak samples is usually done by *weighting* the samples rather than a binary inclusion/exclusion. The AdaBoost algorithm is a well-known example of this class, and can combine weak learners such as decision stumps (a decision tree with only one decision!) into effective classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dec = sklearn.tree.DecisionTreeClassifier(max_depth=1)\n",
    "dec.fit(sonar_train_features, sonar_train_labels)\n",
    "boost_scores = []\n",
    "for estimators in range(50):\n",
    "    boosted_dec = sklearn.ensemble.AdaBoostClassifier(dec, n_estimators=estimators+1, algorithm='SAMME')\n",
    "    boosted_dec.fit(sonar_train_features, sonar_train_labels)\n",
    "    boost_scores.append(boosted_dec.score(sonar_test_features, sonar_test_labels))\n",
    "plt.axhline(dec.score(sonar_test_features, sonar_test_labels))\n",
    "plt.plot(np.arange(len(boost_scores))+1, boost_scores)\n",
    "plt.xlabel('No. estimators')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
