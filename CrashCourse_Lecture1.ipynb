{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section I: Concepts\n",
    "\n",
    "I'm [John Williamson](http://johnhw.com)\n",
    "\n",
    "The purpose of this crash course is to give you enough vocabulary to be able to follow the rest of the summer school. There isn't time to cover the details of the methods I'll talk about, the historical orgins and background or much about what techinques and models you might prefer and why. \n",
    "\n",
    "Instead, I aim to cover just enough that you can understand the material that follows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is machine learning?\n",
    "\n",
    "### Machine learning can be summarised as making *predictions* from *data*\n",
    "This is slightly distinct from statistics, which is traditionally concerned with making inferences from data -- e.g. determining if an effect is present in a scientific study. Instead, machine learning tries to estimate unknown variables given some data which might predict them. \n",
    "\n",
    "### Machine learning in HCI\n",
    "#### Example: gesture recognition\n",
    "#### Example: language modelling\n",
    "#### Example: touch prediction\n",
    "#### Example: sensor fusion\n",
    "\n",
    "### Some mathematical notation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised versus unsupervised learning\n",
    "\n",
    "Supervised learning involves learning a relationship between attribute variables and target variables; in other words learning a function which maps input measurements to target values. This can be in the context of making discrete decisions (is this image a car or not?) or learning continuous relationships (\n",
    "\n",
    "#### Classification\n",
    "#### Regression\n",
    "#### Clustering\n",
    "#### Dimensional reduction\n",
    "#### Manifolds and latent variables\n",
    "#### Learning-to-rank\n",
    "\n",
    "\n",
    "#### Generative models versus black box models\n",
    "##### Tank recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features, labels, data formats\n",
    "\n",
    "### Types of data\n",
    "#### Continuous\n",
    "#### Ordinal\n",
    "#### Categorical\n",
    "### Feature vectors\n",
    "#### Vector length\n",
    "#### Selecting or generating features\n",
    "#### Dealing with time series, text, images, sounds\n",
    "### Labels\n",
    "#### One-hot encoding\n",
    "#### Hyperparameters\n",
    "##### Learning rate\n",
    "##### Smoothness\n",
    "##### Momentum\n",
    "##### Grid search\n",
    "##### Using grid seaches effectively\n",
    "\n",
    "### Learning\n",
    "#### Batches, online learning, mini-batches\n",
    "#### Loss functions\n",
    "#### Gradient descent \n",
    "##### Stochastic gradient descent\n",
    "#### Unbalanced data\n",
    "#### Solutions for unbalanced data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating machine learning\n",
    "\n",
    "\n",
    "#### Generating baselines\n",
    "\n",
    "### Classifiers\n",
    "#### Accuracy\n",
    "#### Why is accuracy not enough?\n",
    "#### FAR, FRR, F1-score\n",
    "#### Receiver-operator curves\n",
    "#### EER, AUC\n",
    "#### Confusion matrices\n",
    "\n",
    "### Unsupervised learning\n",
    "#### Clustering metrics\n",
    "#### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "# force plots to appear inline on this page\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial overfitting\n",
    "A simple example of overfitting can be seen when fitting a polynomial $y=a_0x^0 + a_1x^1 + \\dots + a_qx^q$ to $n$ points. When $n=q$, the curve goes through all the points, but makes wildly inappropriate interpolations between them.\n",
    "\n",
    "The example below shows this effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create some random points\n",
    "n_points = 6\n",
    "x = np.random.uniform(-1,1,(n_points,))\n",
    "y = np.random.uniform(-1,1,(n_points,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the points\n",
    "plt.scatter(x,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a short function to fit and plot a polynomial fit . NumPy has a built in function `polyfit` which does least-squares fitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_poly_fit(x,y,order):    \n",
    "    plt.figure()\n",
    "    plt.scatter(x,y)\n",
    "    # fit a polynomial\n",
    "    coeffs = np.polyfit(x,y,order)\n",
    "    # evaluate it across the domain\n",
    "    xs = np.linspace(np.min(x),np.max(x),100)\n",
    "    ys = np.polyval(coeffs, xs)\n",
    "    plt.plot(xs,ys)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_poly_fit(x,y,order=1) # order=1 means a linear fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the above code with increasing values for the order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel density estimation example\n",
    "We can see the same effect if we try to learn the distribution of data using **kernel density estimation** (KDE). KDE is effectively a smoothed histogram, which is created by \"placing\" smooth distributions on each observed data point and summing them (i.e. convolving them with some window function). This can be used to estimate an underlying smooth distribution from point samples.\n",
    "\n",
    "The key parameter in KDE is the **kernel width** $\\sigma$, which determines how wide each distribution will be and thus how smooth the overall distribution will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate some random numbers -- the use of the  beta distribution isn't important, it just gives an interesting shape\n",
    "x = np.random.beta(0.5, 2, 40);\n",
    "# plot the data points\n",
    "plt.figure()\n",
    "# scatter plot showing the actual positions\n",
    "plt.scatter(x,np.ones_like(x))\n",
    "plt.figure()\n",
    "# histogram of the data points\n",
    "plt.hist(x, normed=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats \n",
    "\n",
    "# plot the kernel density estimate (Gaussian window) with the given bandwidth\n",
    "def plot_kde(x, width):\n",
    "    kde = stats.kde.gaussian_kde(x, bw_method=width)\n",
    "    # evaluate kde estimate over range of x\n",
    "    xs = np.linspace(np.min(x), np.max(x), 100) \n",
    "    plt.figure()\n",
    "    plt.plot(xs, kde(xs))\n",
    "    plt.scatter(x, np.ones_like(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_kde(x, 2) # too smooth\n",
    "plot_kde(x, 1) # a bit too smooth\n",
    "plot_kde(x, 0.5) # good\n",
    "plot_kde(x, 0.1) # too rough\n",
    "plot_kde(x, 0.01) # just learning the data points\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the function approximates the data better, the generalisation performance drops. If we split the data randomly into two portions $X_1$ and $X_2$, and learn the KDE using only $X_1$ (training set) and then compute how likely $X_2$ (test set) is given that learned distribution, we can see this loss of generalisation performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(x):\n",
    "    # our data here is random and uncorrelated, so we can just split the array into two\n",
    "    l = len(x)//2\n",
    "    return x[:l], x[l:]\n",
    "\n",
    "def learn_kde(x, width):\n",
    "    return stats.kde.gaussian_kde(x,width)\n",
    "\n",
    "def evaluate_kde(x, kde):\n",
    "    # we can compute the log-likelihood by summing the log pdf evaluated at x\n",
    "    return np.sum(kde.logpdf(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_kde(x, width):\n",
    "    # split the data into two parts, train on one, and then test it on both of the splits\n",
    "    x1,x2 = split_data(x)\n",
    "    kde = learn_kde(x1, width)\n",
    "    # return the train log-likelihood and test log-likelihood    \n",
    "    return evaluate_kde(x1, kde), evaluate_kde(x2, kde)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_kde_lik(x):\n",
    "    # plot test and train log-likelihood as a function of 1/sigma\n",
    "    widths = np.linspace(0.05, 20, 100)\n",
    "    trains = []\n",
    "    tests = []\n",
    "    # test a bunch of widths\n",
    "    for width in widths:\n",
    "        train, test = test_kde(x,1.0/width)\n",
    "        trains.append(train)\n",
    "        tests.append(test)\n",
    "        \n",
    "    # plot and label\n",
    "    plt.plot(widths, trains)\n",
    "    plt.plot(widths, tests)\n",
    "    plt.xlabel(\"$\\sigma^{-1}$\")\n",
    "    plt.ylabel(\"Log-likelihood\")\n",
    "    plt.legend([\"Training\", \"Test\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the log-likelihood as a function of $\\sigma^{-1}$ (the reciprocal simply makes it easier to see what is going on in the graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_kde_lik(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training error can always be reduced -- but it makes things worse\n",
    "We can see that test and training become better as $\\sigma^{-1}$ approaches 2-4 (the exact value will depend on what random numbers we originally drew), then rapidly decreases; but the training log-likelihood **always** increases as we approximate the original data better and better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data hygiene\n",
    "This is why **data hygiene** is absolutely critical. If you let any part of the data you use to evaluate performance affect the train process your results are *meaningless*. \n",
    "\n",
    "#### Randomised selection\n",
    "One approach to splitting up data is to randomly assign some elements to the training set and some to the test set (e.g. in an image classification task, 70% of the images are assigned to the training class and 30% to the test class). \n",
    "\n",
    "This seems like an unbiased way of separating the data, and it is for problems which are effectively uncorrelated. But imagine we have a time series $x_0, x_1, \\dots, x_n$ and we build our input features $X_0, X_1, \\dots$ by taking overlapping windows of the series. If we randomly choose elements of $X$, many of the elements in the test and training set may be almost identical ( because they appeared next to each other in the time series). This leads to wildly optimistic test results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Block selection\n",
    "A much better approach here is to split the data into a large chunks. Say the data was a series of field recordings of birds taken on 10 different days; the first 6 days might be assigned as training and the last 4 as test. \n",
    "\n",
    "This is much more likely to be a reliable estimator of future performance, because the key idea is to *predict future behaviour* -- to learn what we have not seen. Choosing the split of training and test requires thought and domain knowledge and it is critical to make sure that the evaluation results are meaningful.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "Sometimes the data is too precious to split into training and test sets; there simply isn't enough of it. \n",
    "\n",
    "One approach to getting reliable results without overfitting is to use *cross validation*. This simply involves splitting the data into a test and training set repeatedly and averaging the results. \n",
    "\n",
    "#### k-fold cross validation\n",
    "*k-fold* cross validation splits the data into $k$ blocks (again the block selection strategy needs to be chosen carefully to avoid overfitting) and then trains on $k-1$ of the blocks and test on the remaining one, for each of the $k$ test blocks.\n",
    "#### Leave-one-out cross validation (LOOCV)\n",
    "LOOCV takes this to an extreme, and splits the data into $k=n$ blocks (for $n$ data points) and trains on all but one data point and tests on the one that was left out. This is a reliable estimator of performance but can be very computationally expensive, and the benefits over $k$-fold cross validation are not always great.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# k-fold cross validation with the KDE example\n",
    "\n",
    "def k_split(x, k):\n",
    "    # split the data into even chunks\n",
    "    l = len(x)//k\n",
    "    return [x[i*l:(i+1)*l] for i in range(k)]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_kde_k_fold(x, width, k):\n",
    "    # split the data into two parts, train on one, and then test it on both of the splits\n",
    "    liks = []\n",
    "    folds = k_split(x,k)\n",
    "    for kn in range(k):\n",
    "        # concatenate all folds *but* kn\n",
    "        x1 = np.concatenate([folds[i] for i in range(k) if i!=kn])            \n",
    "        # test set is the one we left out\n",
    "        x2 = folds[kn]\n",
    "        # fit the KDE\n",
    "        kde = learn_kde(x1, width)\n",
    "        # return the log-likelihood for this fold\n",
    "        lik = evaluate_kde(x2, kde)\n",
    "        liks.append(lik)\n",
    "    return liks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now we can plot the performance with one std. dev. bounds\n",
    "\n",
    "def plot_kde_lik_k_fold(x, k):\n",
    "    # plot test and train log-likelihood as a function of 1/sigma\n",
    "    widths = np.linspace(0.05, 20, 100)\n",
    "    means = []\n",
    "    stds = []\n",
    "    # test a bunch of widths\n",
    "    for width in widths:\n",
    "        ks = test_kde_k_fold(x,1.0/width,k)\n",
    "        means.append(np.mean(ks))\n",
    "        stds.append(np.std(ks))\n",
    "                \n",
    "    # plot and label    \n",
    "    means = np.array(means) # convert lists no numpy arrays so we can do arithmetic on them\n",
    "    stds = np.array(stds)\n",
    "    \n",
    "    plt.fill_between(widths, means-stds, means+stds, alpha=0.1)\n",
    "    plt.plot(widths, means)\n",
    "    plt.xlabel(\"$\\sigma^{-1}$\")\n",
    "    plt.ylabel(\"Log-likelihood\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_kde_lik_k_fold(x, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation sets\n",
    "Often your learning algorithms might have hyperparameters you need to estimate (e.g. learning rate). \n",
    "\n",
    "You absolutely **cannot** use the test set to evaluate the performance with various parameter settings and then choose the best one. This will overfit, because the test set is influencing the training performance. Train and test must be **completely** separated; no information may flow from the test data to the training process\n",
    "\n",
    "Instead, you can create a **validation** set from the training set, and use that to tweak the hyperparameters. One common and sensible approach is to first separate off a single, fixed test set, and then use cross-validation to create multiple train and validation sets. \n",
    "\n",
    "*Nested cross-validation* applies cross validation to both the validation and test sets, first fixing a fold split for the test set, then optimising the hyperparameters using cross-validation on the remaining data, then moving onto the next fold split for the test set, and so on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines\n",
    "Let's return to our classification example. What is good performance? Or more particularly, what is bad performance -- what's the baseline? A very simple test of whether the classifier is learning **anything** useful is to simply randomly permute the targets so they no longer correspond to the inputs. \n",
    "\n",
    "This estimates the chance performance of the classification process. It might be that you have a binary classification problem with exactly balanced datasets, so the baseline is 50%; but often the data is unbalanced and there are multiple classes. Permuting the targets is a very quick way to test the random performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-algorithms\n",
    "### Why meta algorithms?\n",
    "### Voting hybrid models\n",
    "#### Weighted models\n",
    "### Bagging\n",
    "### Boosting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
