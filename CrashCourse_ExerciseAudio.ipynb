{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Learning to recognise touch sounds\n",
    "This exercise will look at recognising the audio registered by a piezo contact microphone on a mobile device when different parts of it are touched by a user. This is data from the **Stane** project ([Paper](http://www.dcs.gla.ac.uk/~rod/publications/MurWilHugQua08.pdf) and [video](http://www.dcs.gla.ac.uk/~rod/Videos/i_chi2.mov)), which used 3D printed surfaces to make super-cheap touch controllers.\n",
    "\n",
    "<img src=\"imgs/stane_1.png\" width=\"400px\">\n",
    "<img src=\"imgs/stane_2.png\" width=\"400px\">\n",
    "\n",
    "The machine learning problem is simple: given a set of recordings of a user rubbing discrete touch zones on this 3D printed case, train a classifier which can distinguish which zone is being touched. This is in essence similar to speech recognition, but with a much simpler acoustic problem and no need to deal with language modeling.\n",
    "\n",
    "We will use multi-class classification to distinguish the touch zones from the audio alone. We can assume a small number of discrete touch areas, and that there is no model governing how they might be touched (i.e. touches happen at random).\n",
    "\n",
    "\n",
    "\n",
    "## A data processing pipeline\n",
    "We need to develop a *pipeline* to process the data. There are several stages common to most supervised learning tasks:\n",
    "\n",
    "1. Loading the original data (from files, databases etc.)\n",
    "1. Pre-processing (removing outliers, resampling or interpolating, raw normalisation)\n",
    "1. Feature extraction (transforming data into fixed length feature vectors)\n",
    "1. Feature processing (offset removal and normalisation)\n",
    "1. Data splitting (dividing into testing and training sections)\n",
    "1. Classification (training the classifier)\n",
    "1. Evaluation (testing the classifier performance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading the data\n",
    "The first thing we need to do is to load the data. The data is in `datasets/stane/` and consists of five wave files from scratching five different surfaces, each 60 seconds in length, 4Khz, 16 bit PCM. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wavfile\n",
    "import scipy.signal as sig\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "# force plots to appear inline on this page\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd datasets\\stane\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load each of the files into sound_files\n",
    "sound_files = []\n",
    "for texture in \"12345\":\n",
    "    # load the wavefile\n",
    "    fname = \"stane_%s.wav\" % texture\n",
    "    sr, data = wavfile.read(fname)\n",
    "    print \"Loaded %s, %s samples at %dHz (%f seconds)\" % (fname, len(data), sr, len(data)/float(sr))\n",
    "    sound_files.append(data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has loaded each of the wave files into `sound_files[]`, one for each of our 5 classes. We must process this into fixed length feature vectors which we can feed to a classifier. This is the major \"engineering\" of the machine learning process -- good feature selection is essential to getting good performance.\n",
    "\n",
    "It's important that we can change the parameters of the feature extraction and learning and be able to rerun the entire process in one go. We define a dictionary called `params` which will hold every adjustable parameter and a function called `run_pipeline()` which will run our entire pipeline. For now, it does nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'sample_rate':4096,\n",
    "         }\n",
    "\n",
    "def run_pipeline(sound_files, params):            \n",
    "    # this is the outline of our pipeline\n",
    "    pre_processed = pre_process(sound_files, params)\n",
    "    features, targets = feature_extract(pre_processed, params)\n",
    "    train, validate, test = split_features(features, targets, params)    \n",
    "    classifier = train_classifier(features, targets, params)\n",
    "    evaluate(classifier, features, targets, params)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Pre-processing\n",
    "This data is pretty clean already. We can plot a section of the data to have a look at it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_second = params[\"sample_rate\"]\n",
    "\n",
    "# plot two of the files\n",
    "plot_section_0 = sound_files[0][:one_second] \n",
    "plot_section_1 = sound_files[1][:one_second] \n",
    "\n",
    "# generate time indices\n",
    "timebase = np.arange(len(plot_section_0)) / float(params[\"sample_rate\"])\n",
    "plt.figure()\n",
    "plt.plot(timebase, plot_section_0)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.figure()\n",
    "plt.plot(timebase, plot_section_1)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view this in the frequency domain using `plt.specgram()`. We have to choose an FFT size and overlap (here I used N=256 samples, overlap=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the cmap= just selects a prettier heat map\n",
    "_ = plt.specgram(plot_section_0, NFFT=256, Fs=params[\"sample_rate\"], noverlap=128, cmap=\"gist_heat\")\n",
    "plt.figure()\n",
    "_ = plt.specgram(plot_section_1, NFFT=256, Fs=params[\"sample_rate\"], noverlap=128, cmap=\"gist_heat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing steps\n",
    "Two things we should do in the pre-processing step:\n",
    "1. normalise the data to 0-1 range\n",
    "2. apply bandpass filtering to select frequencies we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bandpass(x, low, high, sample_rate):\n",
    "    # scipy.signal.filtfilt applies a linear filter to data (*without* phase distortion)\n",
    "    # scipy.signal.butter will design a linear Butterworth filter     \n",
    "    nyquist = sample_rate / 2    \n",
    "    b,a = sig.butter(4, [low/float(nyquist), high/float(nyquist)], btype=\"band\")    \n",
    "    return sig.filtfilt(b,a,x)\n",
    "    \n",
    "\n",
    "def pre_process(sound_files, params):    \n",
    "    processed = []\n",
    "    for sound_file in sound_files:\n",
    "        normalised = sound_file / 32768.0\n",
    "        p = bandpass(normalised, params[\"low_cutoff\"], params[\"high_cutoff\"], params[\"sample_rate\"])\n",
    "        processed.append(p)\n",
    "    return processed\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Testing pre-processing\n",
    "We can test this and check it working by plotting the time series and spectrogram before and after. We can create a quick function to plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_second(x, params):\n",
    "    one_second = params[\"sample_rate\"]\n",
    "    plot_section = x[:one_second] \n",
    "    # generate time indices\n",
    "    timebase = np.arange(len(plot_section)) / float(params[\"sample_rate\"])\n",
    "    plt.figure()\n",
    "    plt.plot(timebase, plot_section)\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.figure()\n",
    "    _ = plt.specgram(plot_section, NFFT=256, Fs=params[\"sample_rate\"], noverlap=128, cmap='gist_heat')\n",
    "    plt.ylabel(\"Freq (Hz)\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test the filtering; these are example values only\n",
    "params[\"low_cutoff\"]=100\n",
    "params[\"high_cutoff\"]=1500\n",
    "processed = pre_process(sound_files, params)\n",
    "\n",
    "# plot the results\n",
    "plot_second(sound_files[0], params)\n",
    "plot_second(processed[0], params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "The next step is to make fixed length feature vectors. This requires some assumptions: we have a continuous signal, so how do we split it up? What processing should we apply to transform the data?  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Step 3: Feature extraction\n",
    "## Step 4: Building a classifier\n",
    "## Step 6: Evaluating performance\n",
    "## Step 7: A better classifier\n",
    "## Step 8: Better evaluation methods\n",
    "## Step 9: Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
