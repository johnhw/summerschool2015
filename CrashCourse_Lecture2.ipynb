{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section II: Some fundamentals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import scipy.special\n",
    "import scipy.stats as stats\n",
    "# force plots to appear inline on this page\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and regularisation\n",
    "### Overfitting \n",
    "### Penalisation\n",
    "#### Lasso / ridge regression\n",
    "### Randomised regularisation\n",
    "#### Bootstrap / bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic approaches\n",
    "#### Random variables\n",
    "A *random variable* is a variable that can take on different values; i.e. one that is \"unassigned\". Proability theory allows us to manipulate random variables without having to assign them a specific value.\n",
    "\n",
    "#### Distributions\n",
    "A *probability* distribution defines how likely different states of a random variable are. The probability distribution of a random variable $x$ is written:\n",
    "$$P(x)$$\n",
    "Random variables can be continuous (e.g. the height of a person) or discrete (the value showing on the face of a dice). The distribution of a discrete variable is described with a *probability mass function* (PMF) which gives each outcome a specific value. A continuous variable has a *probability density function* (PDF) which specifies the spread of the probability as a continuous function.\n",
    "\n",
    "A probability distribution must assign probabilities in the range 0 (impossible) to 1 (definite) and the PMF or PDF **must** integrate to exactly 1 as the random variable under consideration must take on *some* value. Note that a **PDF** can have a value >1 if the integral of the function is still = 1. A PMF always has $P(x)<1 \\forall x$\n",
    "\n",
    "\n",
    "#### Advantages of probabilistic modelling\n",
    "#### Disadvantages of probabilistic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the PMF of the sum of two dice rolls\n",
    "def two_dice():\n",
    "    # form the sum of the cross product of these possibilities\n",
    "    roll_two = [i+j for i in range(6) for j in range(6)]\n",
    "    # now plot the histogram\n",
    "    plt.hist(roll_two, normed=True, bins=range(12))\n",
    "    plt.xlabel(\"Sum of rolls x\")\n",
    "    plt.ylabel(\"P(x)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "two_dice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the PDF of the normal distribution\n",
    "def plot_normal():\n",
    "    # plot the normal (Gaussian distibution) along with a set of points drawn from that distribution\n",
    "    x = np.linspace(-4,4,100)\n",
    "    y = stats.norm.pdf(x) # mean 0, std. dev. 1\n",
    "    plt.plot(x,y)\n",
    "    plt.axhline(0, color='k', linewidth=0.2) # axis line\n",
    " \n",
    "    # mark the mean\n",
    "    plt.text(0, 0.51, '$\\mu$')\n",
    "    plt.axvline(0, color='r')\n",
    "    # highlight one std. dev. to the right\n",
    "    plt.axvspan(0,1, facecolor='b', alpha=0.1)\n",
    "    plt.text(1.2, 0.3, '$\\sigma$')\n",
    "    # take 1000 random samples and scatter plot them\n",
    "    samples = stats.norm.rvs(0,1,1000)\n",
    "    plt.scatter(samples, np.full(samples.shape, .2), s=4, c='b', alpha=0.03)\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"$P(x)$\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_normal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint, marginal, conditional\n",
    "The *joint probability* of two random variables is written $$P(x,y)$$ and gives the probability that $x$ and $y$ take the same value simultaneously. The *marginal probability* is the derivation of $P(x)$ from $P(x,y)$ by integrating (summing) over all the possible choices of $y$:\n",
    "$$P(x) = \\int P(x,y) dy$$\n",
    "\n",
    "Two random variables are *independent* if the they do not have any dependence on each other. If this is the case then the joint distribution is just the product of the individual distributions:\n",
    "$p(x,y) = p(x)p(y)$\n",
    "\n",
    "The *conditional probability* of $x$ **given** $y$ is written as $$p(x|y)$$ and can be computed as $$p(x|y) = \\frac{p(x,y)}{p(x)}$. This tells us how likely $x$ is to occur if we already know  (or fix) the value of $y$.\n",
    "\n",
    "The *expected value* of a random variable with respect to a function is the function weighted by the probability at each point:\n",
    "$$E[f(x)] = \\int p(x)f(x) dx$$\n",
    "or\n",
    "$$E[f(x)] = \\sum_{x} p(x)f(x)$$ in the discrete case. \n",
    "\n",
    "\n",
    "These concepts are easier to see visually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def joint_marginal(cov):\n",
    "    # create an independent 2D normal distribution\n",
    "    x,y = np.meshgrid(np.linspace(-3,3,50), np.linspace(-3,3,50))\n",
    "    pos = np.empty(x.shape + (2,))\n",
    "    pos[:,:,0] = x\n",
    "    pos[:,:,1] = y\n",
    "    joint_pdf = stats.multivariate_normal.pdf(pos, [0,0], cov)\n",
    "    fig = plt.figure()\n",
    "    # plot the joint\n",
    "    ax = fig.add_subplot(2,2,1)\n",
    "    ax.axis('equal')\n",
    "    ax.pcolor(x,y,joint_pdf, cmap='gist_heat')\n",
    "    # plot the marginals\n",
    "    ax = fig.add_subplot(2,2,3)\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    ax.plot(x[0,:], np.sum(joint_pdf, axis=0))\n",
    "    ax = fig.add_subplot(2,2,2)\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    ax.plot(np.sum(joint_pdf, axis=1), x[0,:])\n",
    "    # plot p(x|y)\n",
    "    ax = fig.add_subplot(2,2,4)\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    marginal = np.tile(np.sum(joint_pdf, axis=0), (joint_pdf.shape[0],1))\n",
    "    ax.pcolor(x,y,joint_pdf/marginal, cmap='gist_heat')\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x and y indepependent\n",
    "joint_marginal([[1,0], [0,1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x and y covary\n",
    "joint_marginal([[0.5,0.5], [0.5,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can draw samples from a distribution, which gives us a set of definite (non-random) variables which are distributed according to the PDF or PMF. The mean $\\mu$ of a set of samples from a distribution is an estimate of the expectation, which improves as the number of samples $n$ increases. If we apply a function $f(x)$ to the samples, the arithmetic mean is an estimate of $E[f(x)]$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal distribution\n",
    "The normal distribution (above) is very widely used as a model of random variables. It is fully specified by its mean $\\mu$ and the covariance matrix $\\Sigma$. If you imagine the normal distribution to be a ball shaped mass in space, the mean *translates* the mass, and covariance applies a transformation matrix (scale, rotate and shear) to the ball. [The exact transformation matrix which does this transform is $\\Sigma^{-1/2}$].\n",
    "\n",
    "#### Normal modelling\n",
    "It seems that this might be a very limiting choice but there are two good reasons for this:\n",
    "1. Normal variables have very nice mathematical properties and are easy to work with analyitically (i.e. without relying on numerical computation).\n",
    "2. The *central limit theory* tells us that any sum of random variables (however they are distributed) will tend to a *Levy stable distribution* as the number of variables being summed increases. For most random variables encountered, this means the normal distribution (one specific Levy stable distribution).\n",
    "\n",
    "#### Tails\n",
    "Obviously, normal variables cannot model multi-modal distributions well (where the distribution has multiple \"spikes\"), although mixtures of normal variables can be used to model these. The main issue with normal approximations is that the *tails* (the low probability areas at the extremes of the function) are very \"light\" -- the normal distribution gives very little probability to rare events. Other distributions, such as the *t-distribution* can model phenomena where there are significant rare events. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clt():\n",
    "    # demonstrate the central limit theorem\n",
    "    for i in range(8):\n",
    "        x = np.zeros((800,))\n",
    "        # add i copies of samples drawn from uniform (flat) distribution together\n",
    "        for j in range(i+1):\n",
    "            x += np.random.uniform(-0.5,0.5, x.shape)\n",
    "        plt.figure()\n",
    "        plt.hist(x, bins=np.linspace(-4,4,40), normed=True)\n",
    "        plt.xlabel(\"$x$\")\n",
    "        plt.ylabel(\"$P(x)$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare normal and t-distributed values\n",
    "def plot_normal_and_t():\n",
    "    x = np.linspace(-8, 8,100)\n",
    "    plt.figure()\n",
    "    plt.plot(x, stats.norm.pdf(x))\n",
    "    plt.plot(x, stats.t.pdf(x,1)) # t-distribution with degrees-of-freedom = 1\n",
    "    # fill the difference in the tails\n",
    "    plt.fill_between(x, stats.t.pdf(x,1), stats.norm.pdf(x), where=stats.t.pdf(x,1)>stats.norm.pdf(x), alpha=0.1)\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"$p(x)$\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, stats.norm.logpdf(x))\n",
    "    plt.plot(x, stats.t.logpdf(x,1)) # t-distribution with degrees-of-freedom = 1\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"$\\log p(x)$\")\n",
    "    \n",
    "plot_normal_and_t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability theory\n",
    "#### Probability as a calculus of belief\n",
    "#### Axioms of probability\n",
    "#### Bayes' Rule\n",
    "#### Prior, likelihood, posterior\n",
    "#### Integration over the evidence\n",
    "##### Monte Carlo approximations\n",
    "### Example: language modelling\n",
    "#### n-gram character models\n",
    "#### Bigram matrix\n",
    "#### Sampling from the model\n",
    "#### Evaluating likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information theory\n",
    "### Entropy\n",
    "#### Shannon's law\n",
    "#### Entropy examples\n",
    "#### Mutual information\n",
    "### Example: Fitts' law as an information theoretic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-dimensional spaces\n",
    "### What is a high-dimensional space?\n",
    "\n",
    "### Dealing with high-D problems\n",
    "#### Pre-training\n",
    "#### Heavier tailed distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The volume of a n-D sphere with radius $1/2$  is $$ V_n(R) = \\frac{\\pi^{n/2}}{\\Gamma({n/2}+1)}\\frac{1}{2}^n$$ (i.e. inscribed in a hypersphere). The volume of a unit cube is $$1^n=1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sphere_volume(n):\n",
    "    return 0.5**n * np.pi**(n/2.0) / scipy.special.gamma(n/2.0+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.arange(0,20)\n",
    "plt.plot(x, [sphere_volume(xi) for xi in x])\n",
    "plt.xlabel(\"Dimension\")\n",
    "plt.ylabel(\"Volume\")\n",
    "plt.figure()\n",
    "plt.semilogy(x, [sphere_volume(xi) for xi in x])\n",
    "plt.xlabel(\"Dimension\")\n",
    "plt.ylabel(\"Volume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate points randomly in a hypersphere. It's hard to visualise the hypersphere, but we can show the radii of points on a 2D circle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sphere_points(n, d):\n",
    "    # generate points on the unit circle (uniformly)    \n",
    "    xn = np.random.normal(0,1,(n,2))\n",
    "    r = np.sqrt(np.sum(xn**2, axis=1))    \n",
    "    surface_points = (xn.T/r).T\n",
    "    \n",
    "    # generate points on the unit d-dimensional hypershphere (uniformly)    \n",
    "    xv = np.random.normal(0,1,(n,d))\n",
    "    r_d = np.sqrt(np.sum(xv**2, axis=1))    \n",
    "    d_surface_points = (xv.T/r_d).T\n",
    "    \n",
    "    # generate points on the unit line\n",
    "    xt = np.random.normal(0,1,(n,1))\n",
    "                \n",
    "    # radii of points uniformly distributed in a n-d hypersphere\n",
    "    # can be drawn by sampling using the formula below\n",
    "    # [see: http://math.stackexchange.com/questions/87230/picking-random-points-in-the-volume-of-sphere-with-uniform-probability?rq=1 ]\n",
    "    radius = np.random.uniform(0, 1, n) ** (1.0/d) * 0.5\n",
    "    return (surface_points.T*radius).T, radius, (d_surface_points.T*radius).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a function to plot this. We'll show the 1D distribution of radii, the distribution of radii as if they were on a circle and the true 2D projection of the hypersphere points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_sphere_density(d):\n",
    "    sphere_pts, line_pts, hyp_pts = sphere_points(2000,d)\n",
    "    \n",
    "    # plot the 2D sphere at the corresponding radius\n",
    "    plt.scatter(sphere_pts[:,0], sphere_pts[:,1], alpha=0.5, s=2)    \n",
    "    # plot 1D points at the corresponding radius\n",
    "    plt.scatter(line_pts, np.zeros_like(line_pts), c='g', alpha=0.1, s=2)    \n",
    "    # plot the 2D projection of the hypersphere points\n",
    "    plt.scatter(-line_pts, np.zeros_like(line_pts), c='g', alpha=0.1, s=2)\n",
    "    if d>1:\n",
    "        plt.scatter(hyp_pts[:,0], hyp_pts[:,1], c='r', alpha=0.5, s=2)\n",
    "    plt.axis(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_sphere_density(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_sphere_density(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_sphere_density(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_sphere_density(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_sphere_density(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "If you look at any 2D slice of a hypersphere, the points will appear to be concentrated at the centre as the dimension increases. But almost all of the volume is actually in a very thin shell at the outside of the sphere. This is quite counter-intuitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel trick: a useful high-d space\n",
    "### Kernel basics\n",
    "### Linear decisions in kernel space\n",
    "### Kernel functions\n",
    "### Non-real data: bag-of-words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with time-series\n",
    "### Windowing\n",
    "### Delay embedding\n",
    "### Derivative information\n",
    "\n",
    "### Stateful algortihms\n",
    "#### Hidden markov model\n",
    "#### Kalman filter\n",
    "#### Particle filter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
